
\section{Overview of the Library}\label{sec:lib}
The LAProof library provides formal proofs of error bounds derived from well understood error analyses~\cite{demmel_book,higham_book} for the basic linear algebra operations listed in Tables \ref{tab:vecops}, \ref{tab:matvecops}, and \ref{tab:matops}. The error bounds for each operation are parameterized by the precision of the standard IEEE 754 floating-point formats supported by the Flocq library~\cite{flocq}, and are derived using the standard rounding error model for floating-point
arithmetic ~\cite[sect 2.2]{higham_book}:
\begin{align}
\fl(a \ op \ b) = (a \ & op \ b)(1 + \delta) + \epsilon
\label{eq:errormodel} \\
\quad |\delta| \le u, \ |\epsilon| \le \eta, \ \delta \epsilon =0 &, \ op \in \{+, -, \times, \div \} \nonumber.
\end{align}
Our formal error bounds are derived in Coq, relying on the Flocq library's machine-checked proofs
\cite{flocq} that this standard error model holds for floating-point arithmetic.  Throughout the paper, for a floating-point number with precision $p$ and maximum exponent $e$, we denote the unit roundoff by $u =2^{-p} $, the underflow threshold by $\eta = 2^{3-e-p-1}$, and the exactly representable numbers in the format by  $\mathbb{F}_{p,e}$.

The LAProof error analysis for each operation is performed by writing two pure functional programs in Gallina, the functional programming language embedded in Coq: a real-valued function $\phi_\mathbb{R}(x)$ defined over Coq's axiomatic real numbers that represents the operation in exact arithmetic, and a floating-point valued function $\phi_{\mathbb{F}_{p,e}}(x)$ defined over the IEEE 754  format specified by Flocq.  Using these functional programs, the absolute forward error $F$ is expressed as 
\begin{equation}
F \triangleq | \phi_\mathbb{R}(x) - \phi_{\mathbb{F}_{p,e}}(x)| .
\end{equation}
The mixed backward-forward error requires deriving a suitable perturbation  $\Delta x$  to the inputs of $\phi_{\mathbb{R}}$ and  small forward error term $ \hat{\delta}$ such that
\begin{equation}
 \phi_{\mathbb{F}_{p,e}}(x) = \phi_\mathbb{R} (x + \Delta x) + \hat{\delta}. \label{eq:BE}
\end{equation}
The error bounds in LAProof are expressed using the functions $h(n)$ and
$g(n,m)$ to represent the accumulation of error from
rounding normal and denormal numbers, respectively:
\begin{align}
    h(n) = ((1 + u) ^n - 1). \label{eq:hdef} \\
    g(n,m)= n\eta (1 + h(m)). \label{eq:gdef}
\end{align}

\subsection{Vector Operations}
The core vector operations in LAProof are the inner
(dot) product ($r \leftarrow x \cdot y$), vector addition ($r\leftarrow  x +  y )$,  summation $(r \leftarrow
\sum_{i} x_i)$, and scaling by a constant ($r \leftarrow \alpha x$) . We provide mixed backward-forward error bounds
for the inner product and scaling by a constant. Given that
addition and subtraction are exact for denormal numbers~\cite{hauser96}, we provide a strict backward error bound for summation and  vector addition. Error analyses for scaled vector addition ($r \leftarrow \alpha x  + \beta y$) and the vector norms listed in Table \ref{tab:vecops} follow by composing error bounds of the core vector operations. In the remainder of this section, we sketch the error analyses  formalized in LAProof for the inner product and summation, and discuss some useful corollaries.

\begin{table}[htbp]
\caption{LAProof Vector Operations}
\label{tab:vecops}
\begin{center}
\setlength{\tabcolsep}{0.5em} % for the horizontal padding
{\renewcommand{\arraystretch}{1.4}% for the vertical padding
\begin{tabular}{|c|c|}
\hline
     DOT & $r \leftarrow x \cdot  y$   \\
\hline
     sVec & $r \leftarrow \alpha x$  \\
\hline
     SUM & $ r \leftarrow \sum_{i} x_i $   \\
\hline
     VecAdd & $r \leftarrow  x + y$  \\
\hline
\hline
     VecAXPBY & $r \leftarrow \alpha x + \beta y$  \\
\hline
     VecNRM1 & $r \leftarrow \|x\|_1$   \\ 
     VecNRM2 &  $r \leftarrow \|x\|_2 $   \\ 
     \hline
     \end{tabular} }
\end{center}
\end{table}


LAProof provides a formal proof of the following mixed backward-forward error bound for
the inner product of two vectors assuming the absence of
overflow.

\begin{theorem}[\textbf{bfDOT}] For any two vectors
${\textbf{u}}, {\textbf{v}} \in \mathbb{F}_{p,e}^n$,
the vectors $\hat{\textbf{u}}, \boldsymbol{\delta} \in
\mathbb{R}^n$ and real number $c \in
\mathbb{R}$ exist such that
\begin{align}\fl({\textbf{u}} \cdot {\textbf{v}}) =
\hat{\textbf{u}} \cdot \textbf{v} + {{c}} ,\label{eq:bfDOT}
\end{align} 
where $|c| \le
g (n,n)$ and every $k^{th}$ element of $\hat{\vc{u}}$ respects
the bound $ {\hat{u}}_k= u_k(1 + \delta_k)$ with $|\delta_k| \le
h(n)$.
\label{thm:bfDOT}
\end{theorem}

\begin{proof}
The most common exact-arithmetic version of the inner product
computation loops over the elements of $\vc{u}$ and $\vc{v}$
to accumulate the partial sums $s_k= s_{k-1} + u_k v_k$,
starting from $s_1 = u_1 v_1$.  In floating-point, we have
$\tilde{s}_1 = u_1 v_1 (1+ \delta_1) + \epsilon_1$ for $k = 1$,
and
\begin{align}
  \tilde{s}_k
= (\tilde{s}_{k-1} + u_k v_k (1+\delta_k) + \epsilon_k)
(1+\gamma_k) \label{eq:partialsum}
\end{align}
with $|\delta_k| \le u$, $|\gamma_k| \le u$, and
$|\epsilon_k| \le \eta$.  If we define $\gamma_1 = 0$, we have
\[
  \tilde{s}_k =
\sum_{j=1}^k \left[ \left( u_j v_j (1 + \delta_j) + \epsilon_j
\right)
    \prod_{\ell = j}^k (1 + \gamma_\ell) \right].
\]
Now define
\begin{align*}
\tilde{\gamma}_j = \prod_{\ell=j}^n (1 + \gamma_\ell) - 1, \quad
\tilde{\delta}_j = (1 + \delta_j) (1 + \tilde{\gamma}_j) - 1 
\end{align*}
for which we have the bounds
\begin{align*}
|\tilde{\gamma}_j| \leq h(n-j) \leq h(n-1) , \quad
|\tilde{\delta}_j| \leq h(n-j+1) \leq h(n).
\end{align*}
The computed dot product is
\[
  \tilde{s}_n =
    \sum_{j=1}^n u_j v_j (1+\tilde{\delta}_j) +
    \sum_{j=1}^n \epsilon_j (1 + \tilde{\gamma}_j),
\]
or, equivalently
\[
  \tilde{s}_n =
    \hat{\vc{u}} \cdot \vc{v } + c
\]
where $|c| \leq g(n,n)$ and
$\hat{u}_j = u_j (1 + \tilde{\delta}_j)$. This is a mixed error
bound
because it combines a backward error term
$\hat{u}_j$ and a forward error term $c$.
\end{proof}

In the absence of underflow, using a linear approximation to
the error function $h$ reduces the bound in Theorem
\ref{thm:bfDOT} to that given in the literature~\cite[sec
3.1]{higham_book}.

We prove the following as corollaries to Theorem
\ref{thm:bfDOT}.

\paragraph{Forward error} Given equation (\ref{eq:bfDOT}), the
forward error bound
\begin{align}| \fl({\textbf{u}} \cdot {\textbf{v}}) - ({\vc{u}}
\cdot {\textbf{v}}) | \le S({\vc{u}},{\vc{v}}) h(n) + h(n, n-1)
\label{eq:fDOT} \end{align}  where 
\begin{align}
S(\vc{x}, \vc{y}) = \sum_{i=1}^n |x_i \cdot y_i|
\label{eq:rabsdot}
\end{align}
is straightforward to derive. Linearizing the error function $h$ reduces the forward error
bound to that given in the literature~\cite{blas02_mixed}.

\paragraph{Sparsity} Assuming that one of the vectors
${\textbf{u}}, {\textbf{v}} \in \mathbb{F}_{p,e}^n$
is sparse, we prove a theorem $\textbf{sbfDOT}$, in which the
error functions $g$ and $h$ in Theorem
$\textbf{bfDOT}$ are parameterized by the smallest number of nonzero elements occurring in either vector.

\paragraph{FMA} If the fused multiply-add
operation is used to compute an inner product of the vectors
${\textbf{u}}, {\textbf{v}} \in \mathbb{F}_{p,e}^n$ , the
floating-point partial sums in equation \ref{eq:partialsum}
become \[\tilde{s_k} = \mathrm{FMA}(u_k, v_k, \tilde {s}_{k-1}) =( u_k
v_k + \tilde{s}_{k-1})(1 + \delta_k) + \epsilon_k\] where $|\delta_k|
\le u$ and $|\epsilon_k| \le \eta$. Following a similar analysis
to that of Theorem \ref{thm:bfDOT}, we prove that the FMA inner
product respects the error bounds given in equations
\ref{eq:bfDOT} and \ref{eq:fDOT}.

For summing of elements in a vector, the LAProof library provides the following
backward error bound.

\begin{theorem}[\textbf{bSUM}] For any vector
${\textbf{u}}\in \mathbb{F}_{p,e}^n$,
the vector $\hat{\textbf{u}} \in \mathbb{R}^n$  exists such 
that\begin{align}\fl\left({\sum_{i=1}^n u_i}\right) = 
{\sum_{i=1}^n \hat{u}_i
},\label{eq:fSUM}
\end{align} 
where every $k^{th}$ element of $\hat{\vc{u}}$ respects
the bound \newline $ {\hat{u}}_k= u_k(1 + \delta_k)$ with $|\delta_k| \le
h(n-1)$.
\label{thm:fSUM}
\end{theorem}

\paragraph{Order of summation} Theorem \ref{thm:fSUM} 
holds as an upper bound on the absolute forward error for any permutation of the
vector ${\textbf{u}}$. At a low level, the formalization of
vectors in LAProof uses Coq lists, for which is it easy to prove
this fact.

\subsection{Matrix-Vector Operations}\label{sec:matvec}
The core matrix-vector operation provided to clients of  the LAProof library is the matrix-vector product ($r \leftarrow A x$). An  error bound for the scaled matrix-vector product ($ r  \leftarrow \alpha A x $)  is also provided. The error bound for matrix-vector multiplication follows from the mixed backward-forward error bound for the inner product given in the previous section, following the standard analysis~\cite[sec 3.5]{higham_book}. Assuming the absence of overflow, LAproof provides the following error bound for matrix-vector multiplication.  

\begin{theorem}[\textbf{bfMV}] For any vector
${\textbf{v}} \in \mathbb{F}_{p,e}^n$,
and matrix ${\bm{A}} \in \mathbb{F}_{p,e}^{m\times n}$,  there exists a matrix ${\bm{\Delta A}} \in \mathbb{R}^{m\times n}$ and vector ${\bm{\eta}} \in \mathbb{R}^n$ such that
\begin{align}\fl({\textbf{A}} {\textbf{v}}) = (\textbf{A} + \bm{\Delta A}) \bm{v} + {\bm{\eta}} ,\label{eq:bfMV}
\end{align} 
where every element of the backward error term $\bm{\eta}$ respects
the bound $| \eta| \le g(n,n) $ and each element of the forward error term $\bm{\Delta A}$ respects the bound
 $|{\Delta A}| \le h(n) | A | $.
\label{thm:bfMV}
\end{theorem}

\begin{table}[htbp]
\caption{LAProof Matrix-Vector Operations}
\label{tab:matvecops}
\begin{center}
\setlength{\tabcolsep}{0.5em} % for the horizontal padding
{\renewcommand{\arraystretch}{1.4}% for the vertical padding
\begin{tabular}{|c|c|}
\hline
    MV & $r \leftarrow A x$   \\
\hline
\hline
sMV &$ r  \leftarrow \alpha A x $   \\
\hline
GEMV &  $r \leftarrow \alpha Ax  + \beta y$   \\ 
\hline
\end{tabular} }
\end{center}
\end{table}

\paragraph{Conditions for the absence of overflow}  In Section \ref{sec:sparse} we demonstrate how the LAProof implementation of  matrix-vector multiplication can connect to concrete implementations of low-level basic linear algebra subprograms. In order to guarantee that the concrete implementation respects the error bound given in  Theorem \ref{thm:bfMV}, the assumption of the absence of overflow must be discharged. The LAProof library guarantees the absence of overflow for matrix-vector multiplication under the following conditions. 

\begin{theorem}[\textbf{finiteMV}] For any vector
${\textbf{v}} \in \mathbb{F}_{p,e}^n$,
and matrix ${\bm{ A}} \in \mathbb{F}_{p,e}^{m\times n}$, if the elements of ${\textbf{v}}$ and ${\bm{ A}}$ are bounded by the square root of
\begin{equation}
 \left(\frac{2^{e} - \eta}{1+u} - g(n,n-1)\right) \left(\frac{1}{1 + n(1+u)^n}\right)   \label{eq:bound} 
\end{equation}
and $g(n+1,n) \le  2^e$, then the floating-point result of $\fl({\textbf{A}} {\textbf{v}})$ is a vector with elements that are finite numbers in the floating-point format $\mathbb{F}_{p,e}$.   \label{thm:bound} 
\end{theorem}
The upper bound in Theorem \ref{thm:bound} is achieved in many settings.
As an example, for binary32 and $n = 10{e6}$ the magnitude of
the elements of $\mathbf{v}$ and $\mathbf{A}$ must be less than $1.8{e16}$.

\subsection{Matrix Operations}\label{sec:mat} 
The core matrix operations in LAProof are the matrix-matrix
product ($R \leftarrow A B$), matrix addition ($R \leftarrow A +  B$), and scaling by a constant ($R \leftarrow \alpha A $ ). The formal rounding error analysis for these operations follows from the mixed backward-forward error bounds for the matrix-vector product, vector addition, and vector scaling. The LAProof library provides a forward error bound for the matrix-matrix product
following the literature~\cite{stewart_book,higham_book}, a backward error bound for matrix addition, and a mixed backward-forward error bound for matrix scaling.  Formal mixed error bounds are also provided for scaled matrix-matrix multiplication ($R \leftarrow \alpha AB$), the addition of a scaled matrices ($R \leftarrow \alpha A  + \beta Y$), and a scaled matrix-matrix product plus a scaled matrix {($R \leftarrow \alpha AX  + \beta Y$).} 

\begin{table}[htbp]
\caption{LAProof Matrix Operations}
\label{tab:matops}
\begin{center}
\setlength{\tabcolsep}{0.5em} % for the horizontal padding
{\renewcommand{\arraystretch}{1.4}% for the vertical padding
\begin{tabular}{|c|c|}
\hline
    sMat & $R \leftarrow \alpha A $   \\
\hline
    MM & $R \leftarrow  A B$   \\
\hline         
MatAdd &  $R \leftarrow A +  B$    \\ 
\hline
\hline
sMM & $R \leftarrow \alpha AB $   \\
\hline
MatAXPBY &  $R \leftarrow \alpha X  + \beta Y$   \\ 
\hline
GEMM &  $R \leftarrow \alpha AX  + \beta Y$   \\ 
\hline
     \end{tabular} }
\end{center}
\end{table}

